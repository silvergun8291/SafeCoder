import os
import sys
import argparse
from pathlib import Path
import re
from dotenv import load_dotenv
from qdrant_client import QdrantClient, models
from qdrant_client.models import Distance, VectorParams
from langchain_upstage.embeddings import UpstageEmbeddings
from langchain.schema.document import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm import tqdm

load_dotenv()

config_dir = Path(__file__).resolve().parent.parent
if str(config_dir) not in sys.path:
    sys.path.insert(0, str(config_dir))

try:
    import config_db

    QDRANT_URL = config_db.QDRANT_URL
    QDRANT_API_KEY = config_db.QDRANT_API_KEY
    UPSTAGE_API_KEY = config_db.UPSTAGE_API_KEY
    TEXT_EMBEDDING_MODEL = config_db.TEXT_EMBEDDING_MODEL
    TEXT_VECTOR_DIMENSION = config_db.TEXT_VECTOR_DIMENSION
except ImportError as e:
    print(f"ğŸš¨ config_db.py ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

os.environ["UPSTAGE_API_KEY"] = UPSTAGE_API_KEY
EMBEDDING_MODEL = UpstageEmbeddings(model=TEXT_EMBEDDING_MODEL)

# ì»¬ë ‰ì…˜ ì´ë¦„
COLLECTIONS = {
    "kisa": "kisa_text_db",
    "owasp": "owasp_text_db",
    "semgrep": "semgrep_rule_db"
}

# ì„ë² ë”© ì„¤ì •
CHUNK_SIZE = 2000
CHUNK_OVERLAP = 500
BATCH_SIZE = 32

# âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìˆ˜ì •ë¨)
DATA_BASE_PATH = Path(__file__).resolve().parent.parent.parent / "data" / "raw" / "text"
KISA_PDF_PATH = DATA_BASE_PATH / "kisa_guidelines"  # âœ… ë³€ê²½ë¨
OWASP_MD_PATH = DATA_BASE_PATH / "owasp_cheatsheet"
SEMGREP_MD_PATH = DATA_BASE_PATH / "semgrep_docs"


def clean_pdf_text(text: str) -> str:
    """PDF í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r'\n\d+\s+PART\s+\d+.*?\n', '\n', text)
    text = re.sub(r'\nPART\s+\d+.*?\n', '\n', text)
    text = re.sub(r'\n\d+:\s*', '\n', text)
    text = re.sub(r'(\d+:){2,}', '', text)
    text = re.sub(r'<br\s*/?>', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def is_useful_chunk(text: str) -> bool:
    """ìœ ìš©í•œ ì²­í¬ íŒë‹¨"""
    txt = text.strip()
    if len(txt) < 300:
        # Semgrep autofix ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê¸¸ì´ê°€ ì§§ì•„ë„ ìœ ì§€
        low = txt.lower()
        if "autofix" in low or "\nfix:" in low or low.startswith("fix:"):
            return True
        return False

    csharp_patterns = [
        r'SqlConnection', r'SqlCommand', r'EventArgs',
        r'string usrinput', r'Request\[', r'Request\.Write',
        r'AntiXss', r'Sanitizer\.Get'
    ]

    if sum(1 for p in csharp_patterns if re.search(p, text)) >= 2:
        return False

    if len(re.findall(r'\n\d+\.\s+[^\n]{5,80}', text)) >= 5:
        return False

    return True


def load_kisa_pdfs(kisa_path: Path) -> list:
    """KISA PDF ë¡œë“œ"""
    print(f"\nğŸ“„ KISA PDF ë¡œë“œ ì¤‘: {kisa_path}")

    if not kisa_path.exists():
        print(f"âŒ ê²½ë¡œ ì—†ìŒ: {kisa_path}")
        print(f"   â„¹ï¸  ì˜ˆìƒ ê²½ë¡œ: {kisa_path}")
        return []

    documents = []
    # âœ… í•˜ìœ„ ëª¨ë“  ë””ë ‰í† ë¦¬ì—ì„œ PDF ì°¾ê¸°
    pdf_files = list(kisa_path.rglob("*.pdf"))

    if not pdf_files:
        print(f"âŒ PDF íŒŒì¼ ì—†ìŒ")
        return []

    print(f"   ì°¾ì€ PDF: {len(pdf_files)}ê°œ")

    for pdf_file in tqdm(pdf_files, desc="PDF ë¡œë”©"):
        lang = 'java' if 'java' in pdf_file.name.lower() else 'python'
        loader = PyPDFLoader(str(pdf_file))
        pages = loader.load()

        for page in pages:
            cleaned_text = clean_pdf_text(page.page_content)
            if len(cleaned_text) < 300:
                continue

            doc = Document(
                page_content=cleaned_text,
                metadata={
                    "source_type": "KISA",
                    "language": lang,
                    "source": pdf_file.name,
                    "page": page.metadata.get("page", 0)
                }
            )
            documents.append(doc)

    print(f"âœ… {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    return documents


def load_owasp_md(owasp_dir: Path) -> list:
    """OWASP Cheatsheet MD íŒŒì¼ ë¡œë“œ"""
    print(f"\nğŸ“„ OWASP Cheatsheet MD ë¡œë“œ ì¤‘: {owasp_dir}")

    if not owasp_dir.exists():
        print(f"âŒ ë””ë ‰í„°ë¦¬ ì—†ìŒ: {owasp_dir}")
        print(f"   â„¹ï¸  ì˜ˆìƒ ê²½ë¡œ: {owasp_dir}")
        return []

    documents = []
    md_files = list(owasp_dir.glob("*.md"))

    if not md_files:
        print(f"âŒ MD íŒŒì¼ ì—†ìŒ")
        return []

    print(f"   ì°¾ì€ MD: {len(md_files)}ê°œ")

    for md_file in tqdm(md_files, desc="MD ë¡œë”©"):
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # Semgrep ë¬¸ì„œëŠ” ì‘ì€ íŒŒì¼ë„ ì¤‘ìš”ë„ê°€ ë†’ì•„ í•„í„°ë§í•˜ì§€ ì•ŠìŒ

            title = md_file.stem.replace('-', ' ').replace('_', ' ').title()

            doc = Document(
                page_content=content,
                metadata={
                    "source_type": "OWASP",
                    "title": title,
                    "source": md_file.name,
                    "file_type": "markdown"
                }
            )
            documents.append(doc)
        except Exception as e:
            print(f"   âš ï¸ {md_file.name}: {e}")

    print(f"âœ… {len(documents)}ê°œ MD íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    return documents


def load_semgrep_md(semgrep_dir: Path) -> list:
    """Semgrep ê·œì¹™ MD íŒŒì¼ ë¡œë“œ"""
    print(f"\nğŸ“„ Semgrep ê·œì¹™ MD ë¡œë“œ ì¤‘: {semgrep_dir}")

    if not semgrep_dir.exists():
        print(f"âŒ ë””ë ‰í„°ë¦¬ ì—†ìŒ: {semgrep_dir}")
        print(f"   â„¹ï¸  ì˜ˆìƒ ê²½ë¡œ: {semgrep_dir}")
        return []

    documents = []
    md_files = list(semgrep_dir.glob("*.md"))

    if not md_files:
        print(f"âŒ MD íŒŒì¼ ì—†ìŒ")
        return []

    print(f"   ì°¾ì€ MD: {len(md_files)}ê°œ")

    for md_file in tqdm(md_files, desc="MD ë¡œë”©"):
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()

            if len(content) < 300:
                continue

            rule_id = md_file.stem
            title = rule_id.replace('-', ' ').replace('_', ' ').title()

            lines = content.split('\n')
            if lines and lines[0].startswith('#'):
                title = lines[0].replace('#', '').strip()

            doc = Document(
                page_content=content,
                metadata={
                    "source_type": "Semgrep",
                    "rule_id": rule_id,
                    "title": title,
                    "source": md_file.name,
                    "file_type": "markdown"
                }
            )
            documents.append(doc)
        except Exception as e:
            print(f"   âš ï¸ {md_file.name}: {e}")

    print(f"âœ… {len(documents)}ê°œ MD íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    return documents


def init_qdrant_collection(client: QdrantClient, collection_name: str, vector_dim: int):
    """Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
    print(f"   ğŸ“¦ ì»¬ë ‰ì…˜ '{collection_name}' ì´ˆê¸°í™”...")

    if client.collection_exists(collection_name):
        client.delete_collection(collection_name)

    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE)
    )


def index_to_qdrant(documents: list, collection_name: str, client: QdrantClient):
    """ë¬¸ì„œë¥¼ Qdrantì— ì„ë² ë”©"""
    if not documents:
        print(f"   âš ï¸ ë¬¸ì„œ ì—†ìŒ")
        return 0

    print(f"   ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    # ì»¬ë ‰ì…˜ë³„ ì²­í‚¹ ì „ëµ
    is_semgrep = collection_name == COLLECTIONS.get("semgrep")
    if is_semgrep:
        # Semgrepì€ ë” ì„¸ë°€í•œ ì²­í‚¹ê³¼ ì‘ì€ íŒŒì¼ í†µì§œ ì—…ë¡œë“œë¥¼ ì‚¬ìš©
        semgrep_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = []
        for doc in documents:
            content = doc.page_content or ""
            if len(content) <= 5000:
                # ì‘ì€ íŒŒì¼ì€ í†µì§œ ì—…ë¡œë“œ
                chunks.append(doc)
            else:
                # í° íŒŒì¼ë§Œ ë¶„í• 
                chunks.extend(semgrep_splitter.split_documents([doc]))
        # ìœ ìš© ì²­í¬ í•„í„° + í‚¤ì›Œë“œ ì˜ˆì™¸ëŠ” is_useful_chunkì—ì„œ ì²˜ë¦¬
        filtered_chunks = [c for c in chunks if is_useful_chunk(c.page_content)]
        # íŒŒì¼ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œìœ¼ë¡œ ê³¼ë„í•œ ì¤‘ë³µ ë°©ì§€ (ì˜ˆ: rule-syntax.md í¸ì¤‘ ì™„í™”)
        MAX_CHUNKS_PER_SOURCE = 30
        buckets = {}
        capped_chunks = []
        for c in filtered_chunks:
            src = (c.metadata or {}).get("source", "")
            cnt = buckets.get(src, 0)
            if cnt < MAX_CHUNKS_PER_SOURCE:
                capped_chunks.append(c)
                buckets[src] = cnt + 1
        filtered_chunks = capped_chunks
    else:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        filtered_chunks = [c for c in chunks if is_useful_chunk(c.page_content)]

    if not filtered_chunks:
        print(f"   âŒ ìœ íš¨í•œ ì²­í¬ ì—†ìŒ")
        return 0

    print(f"   {len(documents)} â†’ {len(chunks)} â†’ {len(filtered_chunks)} ì²­í¬")

    text_list = [c.page_content for c in filtered_chunks]
    vectors = EMBEDDING_MODEL.embed_documents(text_list)

    print(f"   â¬†ï¸ ì„ë² ë”© ì—…ë¡œë“œ ì¤‘...")
    uploaded = 0

    for i in tqdm(range(0, len(filtered_chunks), BATCH_SIZE), desc="ì—…ë¡œë“œ", leave=False):
        batch_chunks = filtered_chunks[i:i + BATCH_SIZE]
        batch_vectors = vectors[i:i + BATCH_SIZE]
        points = []

        for j, doc in enumerate(batch_chunks):
            points.append(
                models.PointStruct(
                    id=i + j + 1,
                    vector=batch_vectors[j],
                    payload={**doc.metadata, "page_content": doc.page_content}
                )
            )

        client.upsert(collection_name=collection_name, points=points, wait=True)
        uploaded += len(points)

    print(f"   âœ… {uploaded}ê°œ ë²¡í„° ìƒì„±")
    return uploaded


def main():
    parser = argparse.ArgumentParser(description="í…ìŠ¤íŠ¸ DB ìƒì„± (KISA/OWASP/Semgrep)")
    parser.add_argument("--semgrep-only", action="store_true", help="Semgrep ê·œì¹™ DBë§Œ ìƒì„±/ì—…ë°ì´íŠ¸")
    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ§ª í…ìŠ¤íŠ¸ DB ìƒì„± (KISA + OWASP + Semgrep MD)")
    print("=" * 70)

    # âœ… ê²½ë¡œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print(f"\nğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
    print(f"   - KISA PDF: {KISA_PDF_PATH}")
    print(f"   - OWASP MD: {OWASP_MD_PATH}")
    print(f"   - Semgrep MD: {SEMGREP_MD_PATH}")

    print(f"\nğŸ”Œ Qdrant ì—°ê²°: {QDRANT_URL}")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=False)
    print("âœ… ì—°ê²° ì„±ê³µ\n")

    results = {}

    if args.semgrep_only:
        print("=" * 70)
        print("ğŸ“ Semgrep ê·œì¹™ DB ìƒì„± (MD íŒŒì¼)")
        print("=" * 70)
        init_qdrant_collection(client, COLLECTIONS["semgrep"], TEXT_VECTOR_DIMENSION)
        semgrep_docs = load_semgrep_md(SEMGREP_MD_PATH)
        results["semgrep"] = index_to_qdrant(semgrep_docs, COLLECTIONS["semgrep"], client)
    else:
        # 1. KISA DB
        print("=" * 70)
        print("ğŸ“ 1. KISA í…ìŠ¤íŠ¸ DB ìƒì„±")
        print("=" * 70)
        init_qdrant_collection(client, COLLECTIONS["kisa"], TEXT_VECTOR_DIMENSION)
        kisa_docs = load_kisa_pdfs(KISA_PDF_PATH)
        results["kisa"] = index_to_qdrant(kisa_docs, COLLECTIONS["kisa"], client)

        # 2. OWASP DB (MD íŒŒì¼)
        print("\n" + "=" * 70)
        print("ğŸ“ 2. OWASP í…ìŠ¤íŠ¸ DB ìƒì„± (Cheatsheet MD)")
        print("=" * 70)
        init_qdrant_collection(client, COLLECTIONS["owasp"], TEXT_VECTOR_DIMENSION)
        owasp_docs = load_owasp_md(OWASP_MD_PATH)
        results["owasp"] = index_to_qdrant(owasp_docs, COLLECTIONS["owasp"], client)

        # 3. Semgrep DB (MD íŒŒì¼)
        print("\n" + "=" * 70)
        print("ğŸ“ 3. Semgrep ê·œì¹™ DB ìƒì„± (MD íŒŒì¼)")
        print("=" * 70)
        init_qdrant_collection(client, COLLECTIONS["semgrep"], TEXT_VECTOR_DIMENSION)
        semgrep_docs = load_semgrep_md(SEMGREP_MD_PATH)
        results["semgrep"] = index_to_qdrant(semgrep_docs, COLLECTIONS["semgrep"], client)

    # ì™„ë£Œ ìš”ì•½
    print("\n" + "=" * 70)
    print("âœ… DB ìƒì„± ì™„ë£Œ")
    print("=" * 70)

    total = 0
    for name, count in results.items():
        print(f"ğŸ“Š {name.upper():8} ({COLLECTIONS[name]:20}): {count:6,}ê°œ ë²¡í„°")
        total += count

    print("-" * 70)
    print(f"ğŸ¯ ì´í•©: {total:,}ê°œ ë²¡í„°")
    print("=" * 70)

if __name__ == "__main__":
    main()